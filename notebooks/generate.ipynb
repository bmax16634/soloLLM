{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d07be1",
   "metadata": {},
   "source": [
    "# Generation Notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Import ====\n",
    "import sys, os, json, torch, tiktoken, textwrap\n",
    "\n",
    "# Add Repo Root\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from models.soloGPT_v1_model import SoloGPT_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a216bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Load Config ====\n",
    "with open(\"../config/soloGPT_v1_config.json\", \"r\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd285875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Set Device ====\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1181341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Load tokenizer ====\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96d94ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoloGPT_v1(\n",
       "  (input_embed): Embedding(50257, 1024)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.15, inplace=False)\n",
       "  )\n",
       "  (decoder): Custom_Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x Custom_DecoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.15, inplace=False)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (linear_layer1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (linear_layer2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.15, inplace=False)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (output_layer): Linear(in_features=1024, out_features=50257, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== Load Model ====\n",
    "model = SoloGPT_v1(config).to(device)\n",
    "model.load_state_dict(torch.load('../outputs/pytorch_model.bin', map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c724ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_new_tokens=100, temperature=1.0, top_k=40):\n",
    "    input_ids = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long).to(device)\n",
    "    generated = input_ids\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(generated)\n",
    "        logits = logits[:, -1, :]  # Only the last token's logits\n",
    "\n",
    "        # Top-k sampling\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, k=top_k, dim=-1)\n",
    "        probs = torch.softmax(top_k_logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        next_token = top_k_indices.gather(-1, next_token)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "    return tokenizer.decode(generated[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f8a9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In a future world,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b6504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a future world, he'll be able to use the \"gauntlet of cards\" â€“ that is, one\n",
      "with a more powerful ability to cast spells that are more powerful than cards\n",
      "you might ever want to use. The first way is to get rid of the \"slimpse\" that's\n",
      "just coming around. In the process, someone might be able to build a monster\n",
      "that can summon one of those minions in another.  The first way to get rid of it\n",
      "is to get rid of all the things you\n"
     ]
    }
   ],
   "source": [
    "# ==== Generate ====\n",
    "output = generate(prompt=prompt, max_new_tokens=100, temperature=1.0, top_k=40)\n",
    "print(textwrap.fill(output, width=80))  # wraps at 80 characters per line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
